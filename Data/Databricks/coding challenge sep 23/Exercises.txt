Exercise 1: Creating a Complete ETL Pipeline using Delta Live Tables
(DLT)

DLT IN PYTHON

import dlt
from pyspark.sql.functions import col

# Step 1: Ingest raw data from CSV files
@dlt.table
def raw_transactions():
    return spark.read.csv("dbfs:/FileStore/streaming/input/customer_transaction.csv", header=True, inferSchema=True)

# Step 2: Apply transformations (calculate total transaction amount)
@dlt.table
def transformed_transactions():
    return (
        dlt.read("raw_transactions")
        .withColumn("TotalAmount", col("Quantity") * col("Price"))
    )

# Step 3: Write the final data into a Delta table
@dlt.table
def final_transactions():
    return dlt.read("transformed_transactions")


DLT in SQL

-- Create Raw Transactions Table
CREATE OR REFRESH LIVE TABLE raw_transactions AS 
SELECT * 
FROM read_csv('dbfs:/FileStore/streaming/input/customer_transaction.csv');

-- Create Transformed Transactions Table
CREATE OR REFRESH LIVE TABLE transformed_transactions AS 
SELECT 
    TransactionID,
    TransactionDate,
    CustomerID,
    Product,
    Quantity,
    Price,
    Quantity * Price AS TotalAmount
FROM 
    delta."/delta/customer_transactions";;

===================================================================================

Exercise 2: Delta Lake Operations - Read, Write, Update, Delete, Merge

1. Read Data from Delta Lake

Pyspark:
# Read data from the Delta table
df = spark.read.format("delta").load("/delta/customer_transactions")
df.show(5)  # Display first 5 rows

SQL:
SELECT * FROM delta.`/delta/customer_transactions` LIMIT 5;

2. Write Data to Delta Lake

#append new transactions:
new_data = [
    (6, "2024-09-06", "C005", "Keyboard", 4, 100),
    (7, "2024-09-07", "C006", "Mouse", 10, 20)
]
new_df = spark.createDataFrame(new_data, ["TransactionID", "TransactionDate", "CustomerID", "Product", "Quantity", "Price"])

# Append to Delta table
new_df.write.format("delta").mode("append").save("/delta/customer_transactions")

3. Update Data in Delta Lake
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/delta/customer_transactions")
delta_table.update(
    condition = "Product = 'Laptop'",
    set = { "Price": "1300" }
)

# Verify the update
df_updated = spark.read.format("delta").load("")
df_updated.filter(col("Product") == "Laptop").show()

4. Delete Data from Delta Lake

delta_table.delete(condition = "Quantity < 3")

5. Merge Data into Delta Lake:

merge_data = [
    (1, "2024-09-01", "C001", "Laptop", 1, 1250),  # Updated Price
    (8, "2024-09-08", "C007", "Charger", 2, 30)    # New Transaction
]
merge_df = spark.createDataFrame(merge_data, ["TransactionID", "TransactionDate", "CustomerID", "Product", "Quantity", "Price"])

# Merge into Delta table
delta_table.alias("t").merge(
    merge_df.alias("s"),
    "t.TransactionID = s.TransactionID"
).whenMatchedUpdate(
    condition = "t.TransactionDate < s.TransactionDate",
    set = { "Price": "s.Price", "Quantity": "s.Quantity" }
).whenNotMatchedInsertAll().execute()

=====================================================================================
Exercise - 3

Delta Lake - History, Time Travel, and Vacuum

1.View Delta Table History:
# Check the transaction history using PySpark
delta_table.history().show()
# Check file details using sql
spark.sql("DESCRIBE DETAIL delta.`/delta/customer_transactions`").show()


2. Perform Time Travel
Retrieve the state from 5 versions ago:

df_time_travel = spark.read.format("delta").option("versionAsOf", 5).load("delta/customer_transactions")
df_time_travel.show()

SELECT * FROM delta.`/delta/customer_transactions` TIMESTAMP AS OF '2024-09-04';

3.Vacuum the Delta Table:

VACUUM delta.`/delta/customer_transactions` RETAIN 7 HOURS;

4. Convert Parquet Files to Delta Files

df_csv = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("dbfs:/FileStore/streaming/input/customer_transactions.csv")

df_csv.write.format("parquet").mode("overwrite").save("/parquet/customer_transactions")

# Convert the Parquet table to Delta
spark.read.format("parquet").load("/parquet/customer_transactions/").write.format("delta").mode("overwrite").save("/delta/orders_converted")

## Convert a Parquet table to Delta using SQL
%sql
CONVERT TO DELTA parquet.`/parquet/customer_transactions`;

=====================================================================================

Exercise 4: Implementing Incremental Load Pattern using Delta Lake
1. Set Up Initial Data

initial_data = [
    (1, "2024-09-01", "C001", "Laptop", 1, 1200),
    (2, "2024-09-02", "C002", "Tablet", 2, 300),
    (3, "2024-09-03", "C001", "Headphones", 5, 50)
]
initial_df = spark.createDataFrame(initial_data, ["TransactionID", "TransactionDate", "CustomerID", "Product", "Quantity", "Price"])

# Write initial data to Delta table
initial_df.write.format("delta").mode("overwrite").save("/delta/transactions")

2. Set Up Incremental Data

incremental_data = [
    (4, "2024-09-04", "C003", "Smartphone", 1, 800),
    (5, "2024-09-05", "C004", "Smartwatch", 3, 200),
    (6, "2024-09-06", "C005", "Keyboard", 4, 100),
    (7, "2024-09-07", "C006", "Mouse", 10, 20)
]
incremental_df = spark.createDataFrame(incremental_data, ["TransactionID", "TransactionDate", "CustomerID", "Product", "Quantity", "Price"])

# Append new transactions
incremental_df.write.format("delta").mode("append").save("/delta/transactions")

3. Implement Incremental Load
Read only new transactions (after 2024-09-03):

new_transactions = df_incremental.filter(col("TransactionDate") > "2024-09-03")

new_transactions.write.format("delta").mode("append").save("/delta/transactions")

4. Monitor Incremental Load

# Check version history
history_df = spark.sql("DESCRIBE HISTORY delta.`/delta/transactions`")
history_df.show()

=====================================================================================

