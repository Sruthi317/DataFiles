from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from delta import *

# Initialize SparkSession
spark = SparkSession.builder.getOrCreate()

# Read the CSV file
df = spark.read.csv("file:/content/sample_data/orders_data.csv", header=True, inferSchema=True)

# Perform transformation
transformed_df = df.withColumn("TotalAmount", col("Quantity") * col("Price")) \
                   .filter(col("Quantity") > 5)

# Write the transformed data to a Delta table
transformed_df.write.format("delta").mode("overwrite").saveAsTable("transformed_orders")

# Read from the Delta table
df = spark.table("transformed_orders")

# Perform aggregation
aggregated_df = df.groupBy("Product").agg({"Quantity": "sum"})

# Write the aggregated data to a Delta table
aggregated_df.write.format("delta").mode("overwrite").saveAsTable("aggregated_orders")